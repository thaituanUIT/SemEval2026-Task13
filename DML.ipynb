{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113636,"databundleVersionId":14080332,"sourceType":"competition"},{"sourceId":12849030,"sourceType":"datasetVersion","datasetId":8126845},{"sourceId":4655,"sourceType":"modelInstanceVersion","modelInstanceId":3445,"modelId":1281},{"sourceId":162777,"sourceType":"modelInstanceVersion","modelInstanceId":138422,"modelId":161088},{"sourceId":162803,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":138440,"modelId":161088},{"sourceId":162809,"sourceType":"modelInstanceVersion","modelInstanceId":138446,"modelId":161088},{"sourceId":162826,"sourceType":"modelInstanceVersion","modelInstanceId":138462,"modelId":161088}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":173.645295,"end_time":"2025-10-23T00:23:20.793792","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-23T00:20:27.148497","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-deps /kaggle/input/bnb-pip/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-11-07T02:49:31.950884Z","iopub.execute_input":"2025-11-07T02:49:31.951154Z","iopub.status.idle":"2025-11-07T02:49:36.062579Z","shell.execute_reply.started":"2025-11-07T02:49:31.951130Z","shell.execute_reply":"2025-11-07T02:49:36.061869Z"},"papermill":{"duration":5.661884,"end_time":"2025-10-23T00:20:37.798231","exception":false,"start_time":"2025-10-23T00:20:32.136347","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/bnb-pip/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\nInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.47.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import bitsandbytes as bnb\nprint(\"bitsandbytes\", bnb.__version__)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T02:49:36.064012Z","iopub.execute_input":"2025-11-07T02:49:36.064796Z","iopub.status.idle":"2025-11-07T02:49:45.387680Z","shell.execute_reply.started":"2025-11-07T02:49:36.064763Z","shell.execute_reply":"2025-11-07T02:49:45.386991Z"},"papermill":{"duration":13.94644,"end_time":"2025-10-23T00:20:51.750196","exception":false,"start_time":"2025-10-23T00:20:37.803756","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"bitsandbytes 0.47.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2025-11-07T02:49:45.388632Z","iopub.execute_input":"2025-11-07T02:49:45.388977Z","iopub.status.idle":"2025-11-07T02:49:45.394737Z","shell.execute_reply.started":"2025-11-07T02:49:45.388959Z","shell.execute_reply":"2025-11-07T02:49:45.393891Z"},"papermill":{"duration":0.012743,"end_time":"2025-10-23T00:20:51.767917","exception":false,"start_time":"2025-10-23T00:20:51.755174","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Prepare Training Data","metadata":{"papermill":{"duration":0.004385,"end_time":"2025-10-23T00:20:51.777254","exception":false,"start_time":"2025-10-23T00:20:51.772869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/train.parquet')\n# test = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/test.parquet')\n\nlangs = train['language'].unique()\n# gens = train['generator'].unique()\n\ndf_trains = []\nfold = 0\nmax_len = 300\nfor lang in langs:\n    fold += 1\n    df = train[train['language'] == lang].reset_index(drop=True)\n    data = []\n    for _, row in df.iterrows():\n        code = row['code'].strip()\n\n        if len(code) > max_len:\n            continue\n            \n        target = row['label']\n\n        text = f\"\"\"<|im_start|>user\nYou are a professional code analyst. Your task is to decide if the following {lang} code snippet was written by an AI model. Respond only with \"Yes\" or \"No\".\n\nHere is the code snippet to classify:\n\"{code}\"\n\nAnswer \"Yes\" if it is AI-generated, otherwise \"No\".<|im_end|>\n<|im_start|>assistant\n<think>\n</think>\nAnswer:\"\"\"\n\n        data.append([code, lang, text, target])\n            \n    df_train = pd.DataFrame(data, columns=['code', 'lang', 'text', 'target'])\n    df_trains.append(df_train)\n\ndf_train = pd.concat(df_trains, axis=0, ignore_index=True)\ndf_train.to_csv(f'train_full.csv', index=False)","metadata":{"papermill":{"duration":2.359946,"end_time":"2025-10-23T00:20:54.141601","exception":false,"start_time":"2025-10-23T00:20:51.781655","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T02:47:27.439143Z","iopub.execute_input":"2025-11-07T02:47:27.439389Z","iopub.status.idle":"2025-11-07T02:47:52.821160Z","shell.execute_reply.started":"2025-11-07T02:47:27.439367Z","shell.execute_reply":"2025-11-07T02:47:52.820340Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\nprint(df_train['text'][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T02:49:22.527222Z","iopub.execute_input":"2025-11-07T02:49:22.527487Z","iopub.status.idle":"2025-11-07T02:49:22.532335Z","shell.execute_reply.started":"2025-11-07T02:49:22.527466Z","shell.execute_reply":"2025-11-07T02:49:22.531601Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>user\nYou are a professional code analyst. Your task is to decide if the following Python code snippet was written by an AI model. Respond only with \"Yes\" or \"No\".\n\nHere is the code snippet to classify:\n\"T = int(input())\nfor t in range(T):\n\tcolor = input().split()\n\ts1 = color[:2]\n\ts2 = color[2:4]\n\ts3 = color[4:6]\n\tbo = False\n\tfor i in range(2):\n\t\tfor j in range(2):\n\t\t\tfor k in range(2):\n\t\t\t\tif s1[i] == s2[j] == s3[k]:\n\t\t\t\t\tbo = True\n\tif bo:\n\t\tprint('YES')\n\telse:\n\t\tprint('NO')\"\n\nAnswer \"Yes\" if it is AI-generated, otherwise \"No\".<|im_end|>\n<|im_start|>assistant\n<think>\n</think>\nAnswer:\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Deep Mutual Learning\nPaper: https://arxiv.org/pdf/1706.00384","metadata":{"papermill":{"duration":0.004462,"end_time":"2025-10-23T00:20:54.150920","exception":false,"start_time":"2025-10-23T00:20:54.146458","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile train.py\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import GradScaler\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, get_cosine_schedule_with_warmup\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import BitsAndBytesConfig\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# ----------------------\n# Config\n# ----------------------\n\nmodel_path_a = '/kaggle/input/qwen2.5-coder/transformers/7b/1'\nmodel_path_b = '/kaggle/input/qwen2.5-coder/transformers/3b/1'\nmodel_path_c = '/kaggle/input/qwen2.5-coder/transformers/1.5b/1'\n    \nlambda_dml = 1           # weight for KL mutual distillation, follows https://arxiv.org/pdf/1706.00384\nnum_epochs = 3\nbatch_size = 2\ngradient_accumulation_steps = 4\nseed = 252\n\ndevice_a = torch.device(\"cuda:0\")\ndevice_b = torch.device(\"cuda:1\")\ndevice_c = torch.device('cuda:1')\n\n# ----------------------\n# Tokenizers (separate for each model)\n# ----------------------\ntokenizer_a = AutoTokenizer.from_pretrained(model_path_a)\ntokenizer_b = AutoTokenizer.from_pretrained(model_path_b)\ntokenizer_c = AutoTokenizer.from_pretrained(model_path_c)\n\ntokenizer_a.padding_side = 'left'\ntokenizer_b.padding_side = 'left'\ntokenizer_c.padding_side = 'left'\n\n# ----------------------\n# Utilities\n# ----------------------\ndef set_seed(seed=318):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass SemEvalDataset(Dataset):\n    def __init__(self, prompts, targets):\n        self.prompts = prompts\n        self.targets = targets\n\n    def __getitem__(self, idx):\n        return self.prompts[idx], self.targets[idx]\n\n    def __len__(self):\n        return len(self.targets)\n\n# ----------------------\n# Model wrapper\n# ----------------------\nclass Net(nn.Module):\n    def __init__(self, model_path, device_index):\n        super(Net, self).__init__()\n        self.config = AutoConfig.from_pretrained(model_path)\n\n        # 4-bit quantization config\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        # Backbone loaded directly onto device_index\n        self.backbone = AutoModel.from_pretrained(\n            model_path,\n            use_cache=False,\n            torch_dtype=torch.float16,\n            quantization_config=bnb_config,\n            device_map=device_index\n        )\n\n        # LoRA for feature extraction\n        peft_config = LoraConfig(\n            task_type=TaskType.FEATURE_EXTRACTION,\n            target_modules='all-linear',\n            bias='none',\n            inference_mode=False,\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.\n        )\n        self.backbone = get_peft_model(self.backbone, peft_config)\n\n        # simple classification head\n        self.head = nn.Linear(self.config.hidden_size, 2, bias=False)\n\n    def forward(self, encodings):\n        out = self.backbone(**encodings).last_hidden_state  # (B, L, H)\n        x = out[:, -1, :]  # use last token\n        return self.head(x)  # (B, 2)\n\n# ----------------------\n# Optimizer helper\n# ----------------------\ndef get_optimizer(model, learning_rate=2e-4, differential_lr=2e-4, weight_decay=0.01):\n    no_decay = ['bias', 'LayerNorm.weight']\n    differential_layers = ['backbone']\n\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [\n                param for name, param in model.named_parameters()\n                if (not any(layer in name for layer in differential_layers))\n                and (not any(nd in name for nd in no_decay))\n            ],\n            \"lr\": learning_rate,\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [\n                param for name, param in model.named_parameters()\n                if (not any(layer in name for layer in differential_layers))\n                and (any(nd in name for nd in no_decay))\n            ],\n            \"lr\": learning_rate,\n            \"weight_decay\": 0,\n        },\n        {\n            \"params\": [\n                param for name, param in model.named_parameters()\n                if (any(layer in name for layer in differential_layers))\n                and (not any(nd in name for nd in no_decay))\n            ],\n            \"lr\": differential_lr,\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [\n                param for name, param in model.named_parameters()\n                if (any(layer in name for layer in differential_layers))\n                and (any(nd in name for nd in no_decay))\n            ],\n            \"lr\": differential_lr,\n            \"weight_decay\": 0,\n        },\n    ]\n    return torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, weight_decay=weight_decay)\n\ndef kl_div_symmetric(logits_src, logits_t1, logits_t2, device_src):\n    target = 0.5 * F.softmax(logits_t1.detach().to(device_src, non_blocking=True), dim=-1) + \\\n             0.5 * F.softmax(logits_t2.detach().to(device_src, non_blocking=True), dim=-1)\n    return F.kl_div(\n        F.log_softmax(logits_src, dim=-1),\n        target,\n        reduction='batchmean'\n    )\n    \n# ----------------------\n# Training (single process, two GPUs, full dataset, no validation)\n# ----------------------\ndef train_singleprocess(num_epochs):\n    # Load full training data\n    train = pd.read_csv('/kaggle/working/train_full.csv')\n    train = train.drop_duplicates(['text']).reset_index(drop=True)\n    \n    # if len(pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')) < 100:\n    #     train = train.head(10)\n\n    train = train.head(100)\n    \n    train_prompts = train['text'].tolist()\n    train_targets = train['target'].astype(int).tolist()\n\n    print('Train size:', len(train_prompts))\n\n    train_dataset = SemEvalDataset(train_prompts, train_targets)\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    # Compute MAX_LEN from training prompts for each tokenizer (99th percentile) and pick max\n    max_len_a = int(np.quantile([len(tokenizer_a(x).input_ids) for x in train_prompts], q=0.99))\n    max_len_b = int(np.quantile([len(tokenizer_b(x).input_ids) for x in train_prompts], q=0.99))\n    max_len_c = int(np.quantile([len(tokenizer_c(x).input_ids) for x in train_prompts], q=0.99))\n\n    MAX_LEN = max(max_len_a, max_len_b, max_len_c)\n    print('Max Len A/B/C', max_len_a, max_len_b, max_len_c, '=> use', MAX_LEN)\n\n    set_seed(seed)\n\n    # Instantiate two models on separate devices\n    model_a = Net(model_path_a, 0)   # device_map=0 -> cuda:0\n    model_b = Net(model_path_b, 1)   # device_map=1 -> cuda:1\n    model_c = Net(model_path_c, 1)   # device_map=1 -> cuda:1\n\n    # Move classification heads to devices explicitly\n    model_a.head = model_a.head.to(device_a)\n    model_b.head = model_b.head.to(device_b)\n    model_c.head = model_c.head.to(device_c)\n\n    optimizer_a = get_optimizer(model_a, learning_rate=2e-4, differential_lr=2e-4, weight_decay=0.01)\n    optimizer_b = get_optimizer(model_b, learning_rate=2e-4, differential_lr=2e-4, weight_decay=0.01)\n    optimizer_c = get_optimizer(model_c, learning_rate=2e-4, differential_lr=2e-4, weight_decay=0.01)\n\n    # Training steps calculation\n    num_update_steps_per_epoch = max(1, len(train_loader) // gradient_accumulation_steps)\n    max_train_steps = num_update_steps_per_epoch * num_epochs\n\n    scheduler_a = get_cosine_schedule_with_warmup(optimizer_a, num_warmup_steps=0, num_training_steps=max_train_steps)\n    scheduler_b = get_cosine_schedule_with_warmup(optimizer_b, num_warmup_steps=0, num_training_steps=max_train_steps)\n    scheduler_c = get_cosine_schedule_with_warmup(optimizer_c, num_warmup_steps=0, num_training_steps=max_train_steps)\n\n    scaler = GradScaler() # single scaler is enought (https://discuss.pytorch.org/t/gradient-scaling-with-multiple-scalers/175976/2)\n\n\n    for epoch in range(num_epochs):\n        model_a.train()\n        model_b.train()\n        model_c.train()\n\n        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}')\n        for step, (batch_prompts, batch_targets) in pbar:\n            # Tokenize separately for each model\n            enc_a = tokenizer_a(\n                batch_prompts,\n                return_tensors='pt',\n                padding='longest',\n                truncation=True,\n                max_length=MAX_LEN\n            )\n            enc_b = tokenizer_b(\n                batch_prompts,\n                return_tensors='pt',\n                padding='longest',\n                truncation=True,\n                max_length=MAX_LEN\n            )\n\n            enc_c = tokenizer_c(\n                batch_prompts,\n                return_tensors='pt',\n                padding='longest',\n                truncation=True,\n                max_length=MAX_LEN\n            )\n            \n            # Move to respective devices\n            enc_a = {k: v.to(device_a, non_blocking=True) for k, v in enc_a.items()}\n            enc_b = {k: v.to(device_b, non_blocking=True) for k, v in enc_b.items()}\n            enc_c = {k: v.to(device_c, non_blocking=True) for k, v in enc_c.items()}\n\n            # labels = torch.tensor(batch_targets, dtype=torch.long)\n            labels = torch.as_tensor(batch_targets, dtype=torch.long)\n            labels_a = labels.to(device_a, non_blocking=True)\n            labels_b = labels.to(device_b, non_blocking=True)\n            labels_c = labels.to(device_c, non_blocking=True)\n\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n                # Forward passes\n                logits_a = model_a(enc_a)\n                logits_b = model_b(enc_b)\n                logits_c = model_c(enc_c)\n                \n                # Supervised cross-entropy losses\n                ce_a = F.cross_entropy(logits_a, labels_a)\n                ce_b = F.cross_entropy(logits_b, labels_b)\n                ce_c = F.cross_entropy(logits_c, labels_c)\n\n\n                kl_a_bc = kl_div_symmetric(logits_a, logits_b, logits_c, device_a)\n                kl_b_ac = kl_div_symmetric(logits_b, logits_a, logits_c, device_b)\n                kl_c_ab = kl_div_symmetric(logits_c, logits_a, logits_b, device_c)\n\n                # Total losses per model\n                loss_a = ce_a + lambda_dml * kl_a_bc\n                loss_b = ce_b + lambda_dml * kl_b_ac\n                loss_c = ce_c + lambda_dml * kl_c_ab\n\n                # Scale for gradient accumulation\n                loss_a = loss_a / gradient_accumulation_steps\n                loss_b = loss_b / gradient_accumulation_steps\n                loss_c = loss_c / gradient_accumulation_steps\n\n            # Backward passes — one per model on its own device\n            scaler.scale(loss_a).backward()\n            scaler.scale(loss_b).backward()\n            scaler.scale(loss_c).backward()\n\n            # Optimizer step\n            if (step + 1) % gradient_accumulation_steps == 0:\n                # Unscale before clipping\n                scaler.unscale_(optimizer_a)\n                scaler.unscale_(optimizer_b)\n                scaler.unscale_(optimizer_c)\n\n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(model_a.parameters(), max_norm=10.0)\n                torch.nn.utils.clip_grad_norm_(model_b.parameters(), max_norm=10.0)\n                torch.nn.utils.clip_grad_norm_(model_c.parameters(), max_norm=10.0)\n\n                # ---- Step for Model A ----\n                scaler.step(optimizer_a)\n                scheduler_a.step()\n                optimizer_a.zero_grad(set_to_none=True)\n            \n                # ---- Step for Model B ----\n                scaler.step(optimizer_b)\n                scheduler_b.step()\n                optimizer_b.zero_grad(set_to_none=True)\n\n             # ---- Step for Model C ----\n                scaler.step(optimizer_c)\n                scheduler_c.step()\n                optimizer_c.zero_grad(set_to_none=True)\n                \n                # ---- Update scaler (once) ----\n                scaler.update()\n\n\n            # Logging\n            if step % 10 == 0 or step == len(train_loader) - 1:\n                pbar.set_postfix({\n                    'loss_a': float(loss_a.item() * gradient_accumulation_steps),\n                    'loss_b': float(loss_b.item() * gradient_accumulation_steps),\n                    'loss_c': float(loss_c.item() * gradient_accumulation_steps),\n\n                    'ce_a': float(ce_a.item()),\n                    'ce_b': float(ce_b.item()),\n                    'ce_c': float(ce_c.item()),\n\n                    'kl_a_bc': float(kl_a_bc.item()),\n                    'kl_b_ac': float(kl_b_ac.item()),\n                    'kl_c_ab': float(kl_c_ab.item())\n                })\n\n    # Save final models\n    model_a.backbone.save_pretrained('backbone_a')\n    torch.save(model_a.head.state_dict(), 'head_a.pt')\n    \n    model_b.backbone.save_pretrained('backbone_b')\n    torch.save(model_b.head.state_dict(), 'head_b.pt')\n\n    model_c.backbone.save_pretrained('backbone_c')\n    torch.save(model_c.head.state_dict(), 'head_c.pt')\n    \n    print('Training finished')\n\n\n# ----------------------\n# Main\n# ----------------------\nif __name__ == '__main__':\n    print(\"PyTorch version:\", torch.__version__)\n    print(\"CUDA available:\", torch.cuda.is_available())\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n\n    if torch.cuda.device_count() < 2:\n        raise RuntimeError(\"This script expects at least 2 GPUs (cuda:0 and cuda:1).\")\n\n    train_singleprocess(num_epochs)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T02:49:51.068375Z","iopub.execute_input":"2025-11-07T02:49:51.068940Z","iopub.status.idle":"2025-11-07T02:49:51.080421Z","shell.execute_reply.started":"2025-11-07T02:49:51.068916Z","shell.execute_reply":"2025-11-07T02:49:51.079612Z"},"papermill":{"duration":0.018036,"end_time":"2025-10-23T00:20:54.173407","exception":false,"start_time":"2025-10-23T00:20:54.155371","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python train.py","metadata":{"execution":{"iopub.status.busy":"2025-11-07T02:49:58.897299Z","iopub.execute_input":"2025-11-07T02:49:58.897537Z","iopub.status.idle":"2025-11-07T02:57:50.009040Z","shell.execute_reply.started":"2025-11-07T02:49:58.897521Z","shell.execute_reply":"2025-11-07T02:57:50.008304Z"},"papermill":{"duration":75.416181,"end_time":"2025-10-23T00:22:09.594236","exception":false,"start_time":"2025-10-23T00:20:54.178055","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"2025-11-07 02:50:08.810164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762483808.980523     108 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762483809.029759     108 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nNumber of GPUs available: 2\nTrain size: 100\nMax Len A/B/C 195 195 195 => use 195\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████████████| 4/4 [02:07<00:00, 31.99s/it]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:55<00:00, 27.95s/it]\nEpoch 1: 100%|█| 50/50 [01:13<00:00,  1.48s/it, loss_a=0.844, loss_b=1.07, loss_\nEpoch 2: 100%|█| 50/50 [01:14<00:00,  1.49s/it, loss_a=0.509, loss_b=0.652, loss\nEpoch 3: 100%|█| 50/50 [01:14<00:00,  1.48s/it, loss_a=0.568, loss_b=0.434, loss\nTraining finished\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Multi-GPU Inference","metadata":{"papermill":{"duration":0.005336,"end_time":"2025-10-23T00:22:09.605786","exception":false,"start_time":"2025-10-23T00:22:09.600450","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/test.parquet')\ntest.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T03:04:21.200087Z","iopub.execute_input":"2025-11-07T03:04:21.200341Z","iopub.status.idle":"2025-11-07T03:04:21.218383Z","shell.execute_reply.started":"2025-11-07T03:04:21.200326Z","shell.execute_reply":"2025-11-07T03:04:21.217617Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Index(['ID', 'code'], dtype='object')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"%%writefile inference.py\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport gc\nimport ctypes\nimport math\nimport numpy as np\nimport pandas as pd\nfrom threading import Thread\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast\n\nfrom transformers import AutoTokenizer, AutoConfig, BitsAndBytesConfig\nfrom peft import AutoPeftModelForFeatureExtraction\n\n# -----------------------\n# Basic environment setup\n# -----------------------\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\ndef clean_memory(deep=True):\n    gc.collect()\n    if deep:\n        try:\n            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n        except Exception:\n            pass\n    torch.cuda.empty_cache()\n\n\n# =====================\n# 1) Read + prepare text\n# =====================\ntest = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/test.parquet')\n\ntest_texts = []\nfor code in test['code']:\n    code_snippet = code.strip()\n    prompt = f\"\"\"<|im_start|>user\nYou are a professional code analyst. Your task is to decide if the following code snippet was written by an AI model. Respond only with \"Yes\" or \"No\".\n\nNow, here is the code snippet to classify:\n\"{code_snippet}\"\n\nAnswer \"Yes\" if it is AI-generated, otherwise \"No\".<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nAnswer:\"\"\"\n    test_texts.append(prompt)\n\ntest['text'] = test_texts\ntest['target'] = -100\n\n# =====================\n# 2) Tokenizer + lengths\n# =====================\n\nmodel_path = '/kaggle/input/qwen2.5-coder/transformers/7b/1'\n    \ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.padding_side = 'left'\n\n# compute tokenized lengths (this may be slow but necessary for binning)\ntest['len'] = test['text'].apply(lambda x: len(tokenizer(x).input_ids))\n\n# =====================\n# 3) Bin-balanced sampling + sort within split\n# =====================\n# Create quantile bins by length (e.g. 10 bins). Adjust q if you want finer/coarser bins.\nn_bins = 10\n# qcut can produce fewer bins if duplicates exist; handle duplicates='drop'\ntest['len_bin'] = pd.qcut(test['len'], q=n_bins, labels=False, duplicates='drop')\n\n# Prepare lists collecting halves from each bin\ntest_0_list = []\ntest_1_list = []\n\n# For reproducibility\nRANDOM_STATE = 42\n\nfor b in sorted(test['len_bin'].unique()):\n    bin_df = test[test['len_bin'] == b].copy()\n    # shuffle within bin to avoid positional bias\n    bin_df = bin_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n    mid = len(bin_df) // 2\n    test_0_list.append(bin_df.iloc[:mid])\n    test_1_list.append(bin_df.iloc[mid:])\n\n# Concatenate per-GPU datasets and sort within each split ascending by length to optimize padding\ntest_0 = pd.concat(test_0_list, ignore_index=True).sort_values('len', ascending=True).reset_index(drop=True)\ntest_1 = pd.concat(test_1_list, ignore_index=True).sort_values('len', ascending=True).reset_index(drop=True)\n\n# If one side is missing some remainder because of odd counts, ensure everyone covered\n# Combine leftovers (rare) - but our split above already assigns all rows.\n# Just sanity-check lengths & tokens\nprint(\"GPU0 rows:\", len(test_0), \"mean_len:\", test_0['len'].mean(), \"total_tokens:\", int(test_0['len'].sum()))\nprint(\"GPU1 rows:\", len(test_1), \"mean_len:\", test_1['len'].mean(), \"total_tokens:\", int(test_1['len'].sum()))\nprint(\"Total rows covered:\", len(test_0) + len(test_1), \"original:\", len(test))\n\n# =====================\n# 4) Dataset + DataLoader (include original index so we can place preds correctly)\n# =====================\nclass SemEvalDataset(Dataset):\n    def __init__(self, df):\n        # df must contain columns: 'text', 'target', 'ID' (or original index)\n        self.texts = df['text'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.row_ids = df['ID'].to_numpy()\n\n    def __getitem__(self, idx):\n        # return prompt (string), target (int), original row id\n        return self.texts[idx], int(self.targets[idx]), int(self.row_ids[idx])\n\n    def __len__(self):\n        return len(self.targets)\n\n\nbatch_size = 16\n\ndataset_0 = SemEvalDataset(test_0)\ndataset_1 = SemEvalDataset(test_1)\n\ndataloader_0 = DataLoader(dataset_0, batch_size=batch_size, shuffle=False, drop_last=False)\ndataloader_1 = DataLoader(dataset_1, batch_size=batch_size, shuffle=False, drop_last=False)\n\ntest_dataloaders = [dataloader_0, dataloader_1]\n\n# =====================\n# 5) Model definition\n# =====================\nclass Net(nn.Module):\n    def __init__(self, base_model_path, trained_backbone_path, load_in_device):\n        super(Net, self).__init__()\n\n        self.config = AutoConfig.from_pretrained(base_model_path)\n\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.backbone = AutoPeftModelForFeatureExtraction.from_pretrained(\n            trained_backbone_path,\n            use_cache=False,\n            torch_dtype=torch.float16,\n            quantization_config=bnb_config,\n            device_map=load_in_device\n        )\n\n        self.head = nn.Linear(self.config.hidden_size, 2, bias=False)\n\n    def forward(self, x):\n        # x is tokenized dict sent directly to backbone\n        x = self.backbone(**x).last_hidden_state[:, -1, :]\n        logits = self.head(x)\n        return logits\n\n\n# =====================\n# 6) Load models on each GPU\n# =====================\ntrained_backbone_path = '/kaggle/working/backbone_a'\ntrained_head_path = '/kaggle/working/head_a.pt'\n\nmodel_1 = Net(base_model_path=model_path,\n              trained_backbone_path=trained_backbone_path,\n              load_in_device='cuda:0')\n\nmodel_2 = Net(base_model_path=model_path,\n              trained_backbone_path=trained_backbone_path,\n              load_in_device='cuda:1')\n\n# load head states (weights_only=True used originally; keep same behavior)\nmodel_1.head.load_state_dict(torch.load(trained_head_path, weights_only=True))\nmodel_2.head.load_state_dict(torch.load(trained_head_path, weights_only=True))\n\n# ensure heads on correct device\nmodel_1.head.to('cuda:0')\nmodel_2.head.to('cuda:1')\n\n# Put models into eval mode (backbone's weights are already on the correct device via device_map)\nmodel_1.eval()\nmodel_2.eval()\n\n# =====================\n# 7) Inference function (collect logits + row_ids to reconstruct order)\n# =====================\ndef get_preds(model, tokenizer, dataloader, device, results_dict):\n    \"\"\"\n    Runs inference for the provided dataloader on `device`.\n    Writes to results_dict a list of (row_id_array, logits_array) tuples in order processed.\n    \"\"\"\n    device_short = device  # e.g., 'cuda:0'\n    collected_row_ids = []\n    collected_logits = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, total=len(dataloader)):\n            batch_prompts, batch_targets, batch_row_ids = batch  # prompts: list[str]\n            # Tokenize batch (returns tensors on CPU)\n            encodings = tokenizer(list(batch_prompts), return_tensors='pt', padding='longest', truncation=False)\n            # Move to device with pin_memory + non_blocking\n            encodings = {k: v.to(device_short, non_blocking=True) for k, v in encodings.items()}\n\n            with autocast(device_type='cuda'):\n                logits = model(encodings)  # should be (B, 2)\n\n            # bring logits back to CPU and store along with row_ids\n            collected_logits.append(logits.detach().cpu())\n            collected_row_ids.append(torch.as_tensor(batch_row_ids))\n\n    if len(collected_logits) > 0:\n        results_dict[device_short] = (torch.cat(collected_row_ids).numpy(), torch.cat(collected_logits).numpy())\n    else:\n        results_dict[device_short] = (np.array([], dtype=int), np.zeros((0, 2), dtype=np.float32))\n\n\n# =====================\n# 8) Run threaded inference on both GPUs\n# =====================\nresults = {}\n\nt0 = Thread(target=get_preds, args=(model_1, tokenizer, test_dataloaders[0], 'cuda:0', results))\nt1 = Thread(target=get_preds, args=(model_2, tokenizer, test_dataloaders[1], 'cuda:1', results))\n\nt0.start()\nt1.start()\n\nt0.join()\nt1.join()\n\n# =====================\n# 9) Reconstruct full logits array in original row order\n# =====================\nn_total = len(test)\n# We'll create a logits array indexed by original row_id values.\n# If row_id values are not 0..N-1, map them via a lookup.\nrow_ids_all = np.concatenate([results['cuda:0'][0], results['cuda:1'][0]])\nlogits_all = np.concatenate([results['cuda:0'][1], results['cuda:1'][1]], axis=0)\n\n# create a mapping row_id -> logits\n# If row_id is unique, we can use a dict or numpy indexing\n# We'll construct a DataFrame and then join with original test rows to ensure ordering\npred_df = pd.DataFrame({\n    'ID': row_ids_all,\n    'logit_0': logits_all[:, 0],\n    'logit_1': logits_all[:, 1],\n})\n\n# Merge predictions back to the original test dataframe (which still has row_id col)\n# Use left join on row_id to preserve original input order, then sort by row_id if needed\nmerged = test[['ID']].merge(pred_df, on='ID', how='left')\n\n# Some sanity checks\nif merged[['logit_0', 'logit_1']].isnull().any().any():\n    # if any missing predictions, fill with zeros (shouldn't happen)\n    merged[['logit_0', 'logit_1']] = merged[['logit_0', 'logit_1']].fillna(0.0)\n    print(\"Warning: some rows missing predictions and were filled with zeros.\")\n\nlogits_tensor = torch.as_tensor(merged[['logit_0', 'logit_1']].values, dtype=torch.float32)\npred_probs = F.softmax(logits_tensor, dim=-1)\npred_labels = torch.argmax(pred_probs, dim=-1).numpy()\n\n# =====================\n# 10) Cleanup + write submission\n# =====================\ndel model_1, model_2\nclean_memory()\n\nsub = test[['ID']].copy()\n# sub['prediction'] = pred_probs[:, 1].numpy()\nsub['prediction'] = pred_labels\nsub = sub.sort_values('ID')\nsub.to_csv('sub_a.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:18:14.898755Z","iopub.execute_input":"2025-11-07T03:18:14.899459Z","iopub.status.idle":"2025-11-07T03:18:14.909601Z","shell.execute_reply.started":"2025-11-07T03:18:14.899430Z","shell.execute_reply":"2025-11-07T03:18:14.908985Z"},"papermill":{"duration":0.018376,"end_time":"2025-10-23T00:22:09.629886","exception":false,"start_time":"2025-10-23T00:22:09.611510","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Overwriting inference.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!python inference.py","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:18:18.879537Z","iopub.execute_input":"2025-11-07T03:18:18.880160Z","iopub.status.idle":"2025-11-07T03:22:40.965568Z","shell.execute_reply.started":"2025-11-07T03:18:18.880138Z","shell.execute_reply":"2025-11-07T03:22:40.964837Z"},"papermill":{"duration":23.354514,"end_time":"2025-10-23T00:22:32.989999","exception":false,"start_time":"2025-10-23T00:22:09.635485","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"2025-11-07 03:18:25.267593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762485505.291582     172 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762485505.298669     172 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGPU0 rows: 498 mean_len: 395.67670682730926 total_tokens: 197047\nGPU1 rows: 502 mean_len: 404.5278884462151 total_tokens: 203073\nTotal rows covered: 1000 original: 1000\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████████████| 4/4 [00:20<00:00,  5.19s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [00:21<00:00,  5.26s/it]\n  0%|                                                    | 0/32 [00:00<?, ?it/s]\n  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 1/32 [00:02<01:02,  2.03s/it]\u001b[A\n  6%|██▊                                         | 2/32 [00:03<00:54,  1.82s/it]\u001b[A\n  9%|████▏                                       | 3/32 [00:05<00:52,  1.82s/it]\u001b[A\n 12%|█████▌                                      | 4/32 [00:07<00:51,  1.83s/it]\u001b[A\n 16%|██████▉                                     | 5/32 [00:09<00:50,  1.88s/it]\u001b[A\n 19%|████████▎                                   | 6/32 [00:11<00:52,  2.00s/it]\u001b[A\n 22%|█████████▋                                  | 7/32 [00:13<00:52,  2.10s/it]\u001b[A\n 25%|███████████                                 | 8/32 [00:16<00:52,  2.20s/it]\u001b[A\n 28%|████████████▍                               | 9/32 [00:18<00:53,  2.33s/it]\u001b[A\n 31%|█████████████▍                             | 10/32 [00:21<00:54,  2.46s/it]\u001b[A\n 34%|██████████████▊                            | 11/32 [00:24<00:53,  2.56s/it]\u001b[A\n 38%|████████████████▏                          | 12/32 [00:27<00:53,  2.67s/it]\u001b[A\n 41%|█████████████████▍                         | 13/32 [00:30<00:53,  2.82s/it]\u001b[A\n 44%|██████████████████▊                        | 14/32 [00:33<00:53,  3.00s/it]\u001b[A\n 47%|████████████████████▏                      | 15/32 [00:37<00:54,  3.21s/it]\u001b[A\n 50%|█████████████████████▌                     | 16/32 [00:41<00:55,  3.45s/it]\u001b[A\n 53%|██████████████████████▊                    | 17/32 [00:45<00:55,  3.70s/it]\u001b[A\n 56%|████████████████████████▏                  | 18/32 [00:50<00:55,  3.95s/it]\u001b[A\n 59%|█████████████████████████▌                 | 19/32 [00:55<00:55,  4.26s/it]\u001b[A\n 62%|██████████████████████████▉                | 20/32 [01:00<00:54,  4.54s/it]\u001b[A\n 66%|████████████████████████████▏              | 21/32 [01:06<00:53,  4.85s/it]\u001b[A\n 69%|█████████████████████████████▌             | 22/32 [01:11<00:51,  5.13s/it]\u001b[A\n 72%|██████████████████████████████▉            | 23/32 [01:18<00:48,  5.40s/it]\u001b[A\n 75%|████████████████████████████████▎          | 24/32 [01:24<00:46,  5.80s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 25/32 [01:32<00:44,  6.38s/it]\u001b[A\n 81%|██████████████████████████████████▉        | 26/32 [01:41<00:43,  7.20s/it]\u001b[A\n 84%|████████████████████████████████████▎      | 27/32 [01:51<00:40,  8.15s/it]\u001b[A\n 88%|█████████████████████████████████████▋     | 28/32 [02:05<00:38,  9.71s/it]\u001b[A\n 91%|██████████████████████████████████████▉    | 29/32 [02:20<00:34, 11.34s/it]\u001b[A\n 94%|████████████████████████████████████████▎  | 30/32 [02:38<00:26, 13.20s/it]\u001b[A\n 97%|█████████████████████████████████████████▋ | 31/32 [03:13<00:19, 19.94s/it]\u001b[A\n100%|███████████████████████████████████████████| 32/32 [03:17<00:00,  6.17s/it]\u001b[A\n100%|███████████████████████████████████████████| 32/32 [03:19<00:00,  6.23s/it]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile inference.py\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport gc\nimport ctypes\nimport math\nimport numpy as np\nimport pandas as pd\nfrom threading import Thread\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast\n\nfrom transformers import AutoTokenizer, AutoConfig, BitsAndBytesConfig\nfrom peft import AutoPeftModelForFeatureExtraction\n\n# -----------------------\n# Basic environment setup\n# -----------------------\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\ndef clean_memory(deep=True):\n    gc.collect()\n    if deep:\n        try:\n            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n        except Exception:\n            pass\n    torch.cuda.empty_cache()\n\n\n# =====================\n# 1) Read + prepare text\n# =====================\ntest = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/test.parquet')\n\ntest_texts = []\nfor code in test['code']:\n    code_snippet = code.strip()\n    prompt = f\"\"\"<|im_start|>user\nYou are a professional code analyst. Your task is to decide if the following code snippet was written by an AI model. Respond only with \"Yes\" or \"No\".\n\nNow, here is the code snippet to classify:\n\"{code_snippet}\"\n\nAnswer \"Yes\" if it is AI-generated, otherwise \"No\".<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nAnswer:\"\"\"\n    test_texts.append(prompt)\n\ntest['text'] = test_texts\ntest['target'] = -100\n\n# =====================\n# 2) Tokenizer + lengths\n# =====================\n\nmodel_path = '/kaggle/input/qwen2.5-coder/transformers/3b/1'\n    \ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.padding_side = 'left'\n\n# compute tokenized lengths (this may be slow but necessary for binning)\ntest['len'] = test['text'].apply(lambda x: len(tokenizer(x).input_ids))\n\n# =====================\n# 3) Bin-balanced sampling + sort within split\n# =====================\n# Create quantile bins by length (e.g. 10 bins). Adjust q if you want finer/coarser bins.\nn_bins = 10\n# qcut can produce fewer bins if duplicates exist; handle duplicates='drop'\ntest['len_bin'] = pd.qcut(test['len'], q=n_bins, labels=False, duplicates='drop')\n\n# Prepare lists collecting halves from each bin\ntest_0_list = []\ntest_1_list = []\n\n# For reproducibility\nRANDOM_STATE = 42\n\nfor b in sorted(test['len_bin'].unique()):\n    bin_df = test[test['len_bin'] == b].copy()\n    # shuffle within bin to avoid positional bias\n    bin_df = bin_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n    mid = len(bin_df) // 2\n    test_0_list.append(bin_df.iloc[:mid])\n    test_1_list.append(bin_df.iloc[mid:])\n\n# Concatenate per-GPU datasets and sort within each split ascending by length to optimize padding\ntest_0 = pd.concat(test_0_list, ignore_index=True).sort_values('len', ascending=True).reset_index(drop=True)\ntest_1 = pd.concat(test_1_list, ignore_index=True).sort_values('len', ascending=True).reset_index(drop=True)\n\n# If one side is missing some remainder because of odd counts, ensure everyone covered\n# Combine leftovers (rare) - but our split above already assigns all rows.\n# Just sanity-check lengths & tokens\nprint(\"GPU0 rows:\", len(test_0), \"mean_len:\", test_0['len'].mean(), \"total_tokens:\", int(test_0['len'].sum()))\nprint(\"GPU1 rows:\", len(test_1), \"mean_len:\", test_1['len'].mean(), \"total_tokens:\", int(test_1['len'].sum()))\nprint(\"Total rows covered:\", len(test_0) + len(test_1), \"original:\", len(test))\n\n# =====================\n# 4) Dataset + DataLoader (include original index so we can place preds correctly)\n# =====================\nclass SemEvalDataset(Dataset):\n    def __init__(self, df):\n        # df must contain columns: 'text', 'target', 'ID' (or original index)\n        self.texts = df['text'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.row_ids = df['ID'].to_numpy()\n\n    def __getitem__(self, idx):\n        # return prompt (string), target (int), original row id\n        return self.texts[idx], int(self.targets[idx]), int(self.row_ids[idx])\n\n    def __len__(self):\n        return len(self.targets)\n\n\nbatch_size = 16\n\ndataset_0 = SemEvalDataset(test_0)\ndataset_1 = SemEvalDataset(test_1)\n\ndataloader_0 = DataLoader(dataset_0, batch_size=batch_size, shuffle=False, drop_last=False)\ndataloader_1 = DataLoader(dataset_1, batch_size=batch_size, shuffle=False, drop_last=False)\n\ntest_dataloaders = [dataloader_0, dataloader_1]\n\n# =====================\n# 5) Model definition\n# =====================\nclass Net(nn.Module):\n    def __init__(self, base_model_path, trained_backbone_path, load_in_device):\n        super(Net, self).__init__()\n\n        self.config = AutoConfig.from_pretrained(base_model_path)\n\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.backbone = AutoPeftModelForFeatureExtraction.from_pretrained(\n            trained_backbone_path,\n            use_cache=False,\n            torch_dtype=torch.float16,\n            quantization_config=bnb_config,\n            device_map=load_in_device\n        )\n\n        self.head = nn.Linear(self.config.hidden_size, 2, bias=False)\n\n    def forward(self, x):\n        # x is tokenized dict sent directly to backbone\n        x = self.backbone(**x).last_hidden_state[:, -1, :]\n        logits = self.head(x)\n        return logits\n\n\n# =====================\n# 6) Load models on each GPU\n# =====================\ntrained_backbone_path = '/kaggle/working/backbone_b'\ntrained_head_path = '/kaggle/working/head_b.pt'\n\nmodel_1 = Net(base_model_path=model_path,\n              trained_backbone_path=trained_backbone_path,\n              load_in_device='cuda:0')\n\nmodel_2 = Net(base_model_path=model_path,\n              trained_backbone_path=trained_backbone_path,\n              load_in_device='cuda:1')\n\n# load head states (weights_only=True used originally; keep same behavior)\nmodel_1.head.load_state_dict(torch.load(trained_head_path, weights_only=True))\nmodel_2.head.load_state_dict(torch.load(trained_head_path, weights_only=True))\n\n# ensure heads on correct device\nmodel_1.head.to('cuda:0')\nmodel_2.head.to('cuda:1')\n\n# Put models into eval mode (backbone's weights are already on the correct device via device_map)\nmodel_1.eval()\nmodel_2.eval()\n\n# =====================\n# 7) Inference function (collect logits + row_ids to reconstruct order)\n# =====================\ndef get_preds(model, tokenizer, dataloader, device, results_dict):\n    \"\"\"\n    Runs inference for the provided dataloader on `device`.\n    Writes to results_dict a list of (row_id_array, logits_array) tuples in order processed.\n    \"\"\"\n    device_short = device  # e.g., 'cuda:0'\n    collected_row_ids = []\n    collected_logits = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, total=len(dataloader)):\n            batch_prompts, batch_targets, batch_row_ids = batch  # prompts: list[str]\n            # Tokenize batch (returns tensors on CPU)\n            encodings = tokenizer(list(batch_prompts), return_tensors='pt', padding='longest', truncation=False)\n            # Move to device with pin_memory + non_blocking\n            encodings = {k: v.to(device_short, non_blocking=True) for k, v in encodings.items()}\n\n            with autocast(device_type='cuda'):\n                logits = model(encodings)  # should be (B, 2)\n\n            # bring logits back to CPU and store along with row_ids\n            collected_logits.append(logits.detach().cpu())\n            collected_row_ids.append(torch.as_tensor(batch_row_ids))\n\n    if len(collected_logits) > 0:\n        results_dict[device_short] = (torch.cat(collected_row_ids).numpy(), torch.cat(collected_logits).numpy())\n    else:\n        results_dict[device_short] = (np.array([], dtype=int), np.zeros((0, 2), dtype=np.float32))\n\n\n# =====================\n# 8) Run threaded inference on both GPUs\n# =====================\nresults = {}\n\nt0 = Thread(target=get_preds, args=(model_1, tokenizer, test_dataloaders[0], 'cuda:0', results))\nt1 = Thread(target=get_preds, args=(model_2, tokenizer, test_dataloaders[1], 'cuda:1', results))\n\nt0.start()\nt1.start()\n\nt0.join()\nt1.join()\n\n# =====================\n# 9) Reconstruct full logits array in original row order\n# =====================\nn_total = len(test)\n# We'll create a logits array indexed by original row_id values.\n# If row_id values are not 0..N-1, map them via a lookup.\nrow_ids_all = np.concatenate([results['cuda:0'][0], results['cuda:1'][0]])\nlogits_all = np.concatenate([results['cuda:0'][1], results['cuda:1'][1]], axis=0)\n\n# create a mapping row_id -> logits\n# If row_id is unique, we can use a dict or numpy indexing\n# We'll construct a DataFrame and then join with original test rows to ensure ordering\npred_df = pd.DataFrame({\n    'ID': row_ids_all,\n    'logit_0': logits_all[:, 0],\n    'logit_1': logits_all[:, 1],\n})\n\n# Merge predictions back to the original test dataframe (which still has row_id col)\n# Use left join on row_id to preserve original input order, then sort by row_id if needed\nmerged = test[['ID']].merge(pred_df, on='ID', how='left')\n\n# Some sanity checks\nif merged[['logit_0', 'logit_1']].isnull().any().any():\n    # if any missing predictions, fill with zeros (shouldn't happen)\n    merged[['logit_0', 'logit_1']] = merged[['logit_0', 'logit_1']].fillna(0.0)\n    print(\"Warning: some rows missing predictions and were filled with zeros.\")\n\nlogits_tensor = torch.as_tensor(merged[['logit_0', 'logit_1']].values, dtype=torch.float32)\npred_probs = F.softmax(logits_tensor, dim=-1)\npred_labels = torch.argmax(pred_probs, dim=-1).numpy()\n\n# =====================\n# 10) Cleanup + write submission\n# =====================\ndel model_1, model_2\nclean_memory()\n\nsub = test[['ID']].copy()\n# sub['prediction'] = pred_probs[:, 1].numpy()\nsub['prediction'] = pred_labels\nsub = sub.sort_values('ID')\nsub.to_csv('sub_b.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:25:58.938934Z","iopub.execute_input":"2025-11-07T03:25:58.939677Z","iopub.status.idle":"2025-11-07T03:25:58.948570Z","shell.execute_reply.started":"2025-11-07T03:25:58.939651Z","shell.execute_reply":"2025-11-07T03:25:58.947846Z"},"papermill":{"duration":0.018513,"end_time":"2025-10-23T00:22:33.015056","exception":false,"start_time":"2025-10-23T00:22:32.996543","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Overwriting inference.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!python inference.py","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:26:03.915485Z","iopub.execute_input":"2025-11-07T03:26:03.916219Z","iopub.status.idle":"2025-11-07T03:28:14.955127Z","shell.execute_reply.started":"2025-11-07T03:26:03.916196Z","shell.execute_reply":"2025-11-07T03:28:14.954326Z"},"papermill":{"duration":22.804245,"end_time":"2025-10-23T00:22:55.825110","exception":false,"start_time":"2025-10-23T00:22:33.020865","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"2025-11-07 03:26:10.444332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762485970.466440     260 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762485970.473132     260 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGPU0 rows: 498 mean_len: 395.67670682730926 total_tokens: 197047\nGPU1 rows: 502 mean_len: 404.5278884462151 total_tokens: 203073\nTotal rows covered: 1000 original: 1000\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.52s/it]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.42s/it]\n  0%|                                                    | 0/32 [00:00<?, ?it/s]\n  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 1/32 [00:01<00:38,  1.23s/it]\u001b[A\n  6%|██▊                                         | 2/32 [00:02<00:29,  1.01it/s]\u001b[A\n  9%|████▏                                       | 3/32 [00:02<00:26,  1.08it/s]\u001b[A\n 12%|█████▌                                      | 4/32 [00:03<00:25,  1.10it/s]\u001b[A\n 16%|██████▉                                     | 5/32 [00:04<00:24,  1.09it/s]\u001b[A\n 19%|████████▎                                   | 6/32 [00:05<00:25,  1.04it/s]\u001b[A\n 22%|█████████▋                                  | 7/32 [00:06<00:25,  1.01s/it]\u001b[A\n 25%|███████████                                 | 8/32 [00:07<00:25,  1.04s/it]\u001b[A\n 28%|████████████▍                               | 9/32 [00:09<00:25,  1.10s/it]\u001b[A\n 31%|█████████████▍                             | 10/32 [00:10<00:25,  1.16s/it]\u001b[A\n 34%|██████████████▊                            | 11/32 [00:11<00:25,  1.21s/it]\u001b[A\n 38%|████████████████▏                          | 12/32 [00:13<00:24,  1.25s/it]\u001b[A\n 41%|█████████████████▍                         | 13/32 [00:14<00:24,  1.31s/it]\u001b[A\n 44%|██████████████████▊                        | 14/32 [00:16<00:24,  1.37s/it]\u001b[A\n 47%|████████████████████▏                      | 15/32 [00:17<00:24,  1.44s/it]\u001b[A\n 50%|█████████████████████▌                     | 16/32 [00:19<00:24,  1.55s/it]\u001b[A\n 53%|██████████████████████▊                    | 17/32 [00:21<00:24,  1.64s/it]\u001b[A\n 56%|████████████████████████▏                  | 18/32 [00:23<00:24,  1.74s/it]\u001b[A\n 59%|█████████████████████████▌                 | 19/32 [00:25<00:24,  1.87s/it]\u001b[A\n 62%|██████████████████████████▉                | 20/32 [00:27<00:24,  2.02s/it]\u001b[A\n 66%|████████████████████████████▏              | 21/32 [00:30<00:23,  2.17s/it]\u001b[A\n 69%|█████████████████████████████▌             | 22/32 [00:33<00:23,  2.33s/it]\u001b[A\n 72%|██████████████████████████████▉            | 23/32 [00:36<00:22,  2.51s/it]\u001b[A\n 75%|████████████████████████████████▎          | 24/32 [00:39<00:21,  2.73s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 25/32 [00:43<00:21,  3.03s/it]\u001b[A\n 81%|██████████████████████████████████▉        | 26/32 [00:47<00:20,  3.42s/it]\u001b[A\n 84%|████████████████████████████████████▎      | 27/32 [00:52<00:19,  3.92s/it]\u001b[A\n 88%|█████████████████████████████████████▋     | 28/32 [00:58<00:18,  4.58s/it]\u001b[A\n 91%|██████████████████████████████████████▉    | 29/32 [01:05<00:15,  5.27s/it]\u001b[A\n 94%|████████████████████████████████████████▎  | 30/32 [01:13<00:12,  6.05s/it]\u001b[A\n 97%|█████████████████████████████████████████▋ | 31/32 [01:29<00:09,  9.12s/it]\u001b[A\n100%|███████████████████████████████████████████| 32/32 [01:30<00:00,  2.82s/it]\u001b[A\n100%|███████████████████████████████████████████| 32/32 [01:32<00:00,  2.89s/it]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"%%writefile inference.py\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport gc\nimport ctypes\nimport math\nimport numpy as np\nimport pandas as pd\nfrom threading import Thread\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast\n\nfrom transformers import AutoTokenizer, AutoConfig, BitsAndBytesConfig\nfrom peft import AutoPeftModelForFeatureExtraction\n\n# -----------------------\n# Basic environment setup\n# -----------------------\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\ndef clean_memory(deep=True):\n    gc.collect()\n    if deep:\n        try:\n            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n        except Exception:\n            pass\n    torch.cuda.empty_cache()\n\n\n# =====================\n# 1) Read + prepare text\n# =====================\ntest = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/test.parquet')\n\ntest_texts = []\nfor code in test['code']:\n    code_snippet = code.strip()\n    prompt = f\"\"\"<|im_start|>user\nYou are a professional code analyst. Your task is to decide if the following code snippet was written by an AI model. Respond only with \"Yes\" or \"No\".\n\nNow, here is the code snippet to classify:\n\"{code_snippet}\"\n\nAnswer \"Yes\" if it is AI-generated, otherwise \"No\".<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nAnswer:\"\"\"\n    test_texts.append(prompt)\n\ntest['text'] = test_texts\ntest['target'] = -100\n\n# =====================\n# 2) Tokenizer + lengths\n# =====================\n\nmodel_path = '/kaggle/input/qwen2.5-coder/transformers/1.5b/1'\n    \ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.padding_side = 'left'\n\n# compute tokenized lengths (this may be slow but necessary for binning)\ntest['len'] = test['text'].apply(lambda x: len(tokenizer(x).input_ids))\n\n# =====================\n# 3) Bin-balanced sampling + sort within split\n# =====================\n# Create quantile bins by length (e.g. 10 bins). Adjust q if you want finer/coarser bins.\nn_bins = 10\n# qcut can produce fewer bins if duplicates exist; handle duplicates='drop'\ntest['len_bin'] = pd.qcut(test['len'], q=n_bins, labels=False, duplicates='drop')\n\n# Prepare lists collecting halves from each bin\ntest_0_list = []\ntest_1_list = []\n\n# For reproducibility\nRANDOM_STATE = 42\n\nfor b in sorted(test['len_bin'].unique()):\n    bin_df = test[test['len_bin'] == b].copy()\n    # shuffle within bin to avoid positional bias\n    bin_df = bin_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n    mid = len(bin_df) // 2\n    test_0_list.append(bin_df.iloc[:mid])\n    test_1_list.append(bin_df.iloc[mid:])\n\n# Concatenate per-GPU datasets and sort within each split ascending by length to optimize padding\ntest_0 = pd.concat(test_0_list, ignore_index=True).sort_values('len', ascending=True).reset_index(drop=True)\ntest_1 = pd.concat(test_1_list, ignore_index=True).sort_values('len', ascending=True).reset_index(drop=True)\n\n# If one side is missing some remainder because of odd counts, ensure everyone covered\n# Combine leftovers (rare) - but our split above already assigns all rows.\n# Just sanity-check lengths & tokens\nprint(\"GPU0 rows:\", len(test_0), \"mean_len:\", test_0['len'].mean(), \"total_tokens:\", int(test_0['len'].sum()))\nprint(\"GPU1 rows:\", len(test_1), \"mean_len:\", test_1['len'].mean(), \"total_tokens:\", int(test_1['len'].sum()))\nprint(\"Total rows covered:\", len(test_0) + len(test_1), \"original:\", len(test))\n\n# =====================\n# 4) Dataset + DataLoader (include original index so we can place preds correctly)\n# =====================\nclass SemEvalDataset(Dataset):\n    def __init__(self, df):\n        # df must contain columns: 'text', 'target', 'ID' (or original index)\n        self.texts = df['text'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.row_ids = df['ID'].to_numpy()\n\n    def __getitem__(self, idx):\n        # return prompt (string), target (int), original row id\n        return self.texts[idx], int(self.targets[idx]), int(self.row_ids[idx])\n\n    def __len__(self):\n        return len(self.targets)\n\n\nbatch_size = 16\n\ndataset_0 = SemEvalDataset(test_0)\ndataset_1 = SemEvalDataset(test_1)\n\ndataloader_0 = DataLoader(dataset_0, batch_size=batch_size, shuffle=False, drop_last=False)\ndataloader_1 = DataLoader(dataset_1, batch_size=batch_size, shuffle=False, drop_last=False)\n\ntest_dataloaders = [dataloader_0, dataloader_1]\n\n# =====================\n# 5) Model definition\n# =====================\nclass Net(nn.Module):\n    def __init__(self, base_model_path, trained_backbone_path, load_in_device):\n        super(Net, self).__init__()\n\n        self.config = AutoConfig.from_pretrained(base_model_path)\n\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.backbone = AutoPeftModelForFeatureExtraction.from_pretrained(\n            trained_backbone_path,\n            use_cache=False,\n            torch_dtype=torch.float16,\n            quantization_config=bnb_config,\n            device_map=load_in_device\n        )\n\n        self.head = nn.Linear(self.config.hidden_size, 2, bias=False)\n\n    def forward(self, x):\n        # x is tokenized dict sent directly to backbone\n        x = self.backbone(**x).last_hidden_state[:, -1, :]\n        logits = self.head(x)\n        return logits\n\n\n# =====================\n# 6) Load models on each GPU\n# =====================\ntrained_backbone_path = '/kaggle/working/backbone_c'\ntrained_head_path = '/kaggle/working/head_c.pt'\n\nmodel_1 = Net(base_model_path=model_path,\n              trained_backbone_path=trained_backbone_path,\n              load_in_device='cuda:0')\n\nmodel_2 = Net(base_model_path=model_path,\n              trained_backbone_path=trained_backbone_path,\n              load_in_device='cuda:1')\n\n# load head states (weights_only=True used originally; keep same behavior)\nmodel_1.head.load_state_dict(torch.load(trained_head_path, weights_only=True))\nmodel_2.head.load_state_dict(torch.load(trained_head_path, weights_only=True))\n\n# ensure heads on correct device\nmodel_1.head.to('cuda:0')\nmodel_2.head.to('cuda:1')\n\n# Put models into eval mode (backbone's weights are already on the correct device via device_map)\nmodel_1.eval()\nmodel_2.eval()\n\n# =====================\n# 7) Inference function (collect logits + row_ids to reconstruct order)\n# =====================\ndef get_preds(model, tokenizer, dataloader, device, results_dict):\n    \"\"\"\n    Runs inference for the provided dataloader on `device`.\n    Writes to results_dict a list of (row_id_array, logits_array) tuples in order processed.\n    \"\"\"\n    device_short = device  # e.g., 'cuda:0'\n    collected_row_ids = []\n    collected_logits = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, total=len(dataloader)):\n            batch_prompts, batch_targets, batch_row_ids = batch  # prompts: list[str]\n            # Tokenize batch (returns tensors on CPU)\n            encodings = tokenizer(list(batch_prompts), return_tensors='pt', padding='longest', truncation=False)\n            # Move to device with pin_memory + non_blocking\n            encodings = {k: v.to(device_short, non_blocking=True) for k, v in encodings.items()}\n\n            with autocast(device_type='cuda'):\n                logits = model(encodings)  # should be (B, 2)\n\n            # bring logits back to CPU and store along with row_ids\n            collected_logits.append(logits.detach().cpu())\n            collected_row_ids.append(torch.as_tensor(batch_row_ids))\n\n    if len(collected_logits) > 0:\n        results_dict[device_short] = (torch.cat(collected_row_ids).numpy(), torch.cat(collected_logits).numpy())\n    else:\n        results_dict[device_short] = (np.array([], dtype=int), np.zeros((0, 2), dtype=np.float32))\n\n\n# =====================\n# 8) Run threaded inference on both GPUs\n# =====================\nresults = {}\n\nt0 = Thread(target=get_preds, args=(model_1, tokenizer, test_dataloaders[0], 'cuda:0', results))\nt1 = Thread(target=get_preds, args=(model_2, tokenizer, test_dataloaders[1], 'cuda:1', results))\n\nt0.start()\nt1.start()\n\nt0.join()\nt1.join()\n\n# =====================\n# 9) Reconstruct full logits array in original row order\n# =====================\nn_total = len(test)\n# We'll create a logits array indexed by original row_id values.\n# If row_id values are not 0..N-1, map them via a lookup.\nrow_ids_all = np.concatenate([results['cuda:0'][0], results['cuda:1'][0]])\nlogits_all = np.concatenate([results['cuda:0'][1], results['cuda:1'][1]], axis=0)\n\n# create a mapping row_id -> logits\n# If row_id is unique, we can use a dict or numpy indexing\n# We'll construct a DataFrame and then join with original test rows to ensure ordering\npred_df = pd.DataFrame({\n    'ID': row_ids_all,\n    'logit_0': logits_all[:, 0],\n    'logit_1': logits_all[:, 1],\n})\n\n# Merge predictions back to the original test dataframe (which still has row_id col)\n# Use left join on row_id to preserve original input order, then sort by row_id if needed\nmerged = test[['ID']].merge(pred_df, on='ID', how='left')\n\n# Some sanity checks\nif merged[['logit_0', 'logit_1']].isnull().any().any():\n    # if any missing predictions, fill with zeros (shouldn't happen)\n    merged[['logit_0', 'logit_1']] = merged[['logit_0', 'logit_1']].fillna(0.0)\n    print(\"Warning: some rows missing predictions and were filled with zeros.\")\n\nlogits_tensor = torch.as_tensor(merged[['logit_0', 'logit_1']].values, dtype=torch.float32)\npred_probs = F.softmax(logits_tensor, dim=-1)\npred_labels = torch.argmax(pred_probs, dim=-1).numpy()\n\n# =====================\n# 10) Cleanup + write submission\n# =====================\ndel model_1, model_2\nclean_memory()\n\nsub = test[['ID']].copy()\n# sub['prediction'] = pred_probs[:, 1].numpy()\nsub['prediction'] = pred_labels\nsub = sub.sort_values('ID')\nsub.to_csv('sub_c.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:28:39.506080Z","iopub.execute_input":"2025-11-07T03:28:39.506722Z","iopub.status.idle":"2025-11-07T03:28:39.516974Z","shell.execute_reply.started":"2025-11-07T03:28:39.506697Z","shell.execute_reply":"2025-11-07T03:28:39.516114Z"},"papermill":{"duration":0.018713,"end_time":"2025-10-23T00:22:55.850537","exception":false,"start_time":"2025-10-23T00:22:55.831824","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Overwriting inference.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!python inference.py","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:28:43.944709Z","iopub.execute_input":"2025-11-07T03:28:43.945006Z","iopub.status.idle":"2025-11-07T03:30:00.766157Z","shell.execute_reply.started":"2025-11-07T03:28:43.944984Z","shell.execute_reply":"2025-11-07T03:30:00.764975Z"},"papermill":{"duration":23.614591,"end_time":"2025-10-23T00:23:19.471375","exception":false,"start_time":"2025-10-23T00:22:55.856784","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"2025-11-07 03:28:49.980100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762486130.002718     291 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762486130.009476     291 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGPU0 rows: 498 mean_len: 395.67670682730926 total_tokens: 197047\nGPU1 rows: 502 mean_len: 404.5278884462151 total_tokens: 203073\nTotal rows covered: 1000 original: 1000\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n  0%|                                                    | 0/32 [00:00<?, ?it/s]\n  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 1/32 [00:00<00:28,  1.10it/s]\u001b[A\n  6%|██▊                                         | 2/32 [00:01<00:18,  1.58it/s]\u001b[A\n  9%|████▏                                       | 3/32 [00:01<00:16,  1.81it/s]\u001b[A\n 12%|█████▌                                      | 4/32 [00:02<00:14,  1.91it/s]\u001b[A\n 16%|██████▉                                     | 5/32 [00:02<00:13,  1.94it/s]\u001b[A\n 19%|████████▎                                   | 6/32 [00:03<00:13,  1.86it/s]\u001b[A\n 22%|█████████▋                                  | 7/32 [00:03<00:13,  1.82it/s]\u001b[A\n 25%|███████████                                 | 8/32 [00:04<00:13,  1.76it/s]\u001b[A\n 28%|████████████▍                               | 9/32 [00:05<00:13,  1.69it/s]\u001b[A\n 31%|█████████████▍                             | 10/32 [00:05<00:13,  1.63it/s]\u001b[A\n 34%|██████████████▊                            | 11/32 [00:06<00:13,  1.57it/s]\u001b[A\n 38%|████████████████▏                          | 12/32 [00:07<00:13,  1.50it/s]\u001b[A\n 41%|█████████████████▍                         | 13/32 [00:08<00:13,  1.44it/s]\u001b[A\n 44%|██████████████████▊                        | 14/32 [00:08<00:13,  1.38it/s]\u001b[A\n 47%|████████████████████▏                      | 15/32 [00:09<00:13,  1.29it/s]\u001b[A\n 50%|█████████████████████▌                     | 16/32 [00:10<00:13,  1.21it/s]\u001b[A\n 53%|██████████████████████▊                    | 17/32 [00:11<00:13,  1.15it/s]\u001b[A\n 56%|████████████████████████▏                  | 18/32 [00:12<00:12,  1.10it/s]\u001b[A\n 59%|█████████████████████████▌                 | 19/32 [00:13<00:12,  1.02it/s]\u001b[A\n 62%|██████████████████████████▉                | 20/32 [00:14<00:12,  1.03s/it]\u001b[A\n 66%|████████████████████████████▏              | 21/32 [00:16<00:12,  1.12s/it]\u001b[A\n 69%|█████████████████████████████▌             | 22/32 [00:17<00:11,  1.20s/it]\u001b[A\n 72%|██████████████████████████████▉            | 23/32 [00:19<00:11,  1.30s/it]\u001b[A\n 75%|████████████████████████████████▎          | 24/32 [00:20<00:11,  1.42s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 25/32 [00:22<00:10,  1.56s/it]\u001b[A\n 81%|██████████████████████████████████▉        | 26/32 [00:24<00:10,  1.74s/it]\u001b[A\n 84%|████████████████████████████████████▎      | 27/32 [00:27<00:09,  1.97s/it]\u001b[A\n 88%|█████████████████████████████████████▋     | 28/32 [00:30<00:09,  2.30s/it]\u001b[A\n 91%|██████████████████████████████████████▉    | 29/32 [00:34<00:08,  2.69s/it]\u001b[A\n 94%|████████████████████████████████████████▎  | 30/32 [00:38<00:06,  3.16s/it]\u001b[A\n 97%|█████████████████████████████████████████▋ | 31/32 [00:47<00:04,  4.97s/it]\u001b[A\n100%|███████████████████████████████████████████| 32/32 [00:48<00:00,  1.52s/it]\u001b[A\n100%|███████████████████████████████████████████| 32/32 [00:49<00:00,  1.53s/it]\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Ensemble","metadata":{"papermill":{"duration":0.006334,"end_time":"2025-10-23T00:23:19.485053","exception":false,"start_time":"2025-10-23T00:23:19.478719","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub_a = pd.read_csv('sub_a.csv')\nsub_b = pd.read_csv('sub_b.csv')\nsub_c = pd.read_csv('sub_c.csv')\n\nsub = sub_a[['ID']].copy()\n\nsub['prediction'] = (\n    0.5*sub_a['prediction'] + 0.3*sub_b['prediction'] + 0.2*sub_c['prediction']\n)\nsub['prediction'] = (sub['prediction'] >= 0.5).astype(int)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:31:50.226550Z","iopub.execute_input":"2025-11-07T03:31:50.227154Z","iopub.status.idle":"2025-11-07T03:31:50.239415Z","shell.execute_reply.started":"2025-11-07T03:31:50.227131Z","shell.execute_reply":"2025-11-07T03:31:50.238683Z"},"papermill":{"duration":0.024656,"end_time":"2025-10-23T00:23:19.516063","exception":false,"start_time":"2025-10-23T00:23:19.491407","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"pred = pd.read_csv('submission.csv')\npred['prediction'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-11-07T03:32:38.792558Z","iopub.execute_input":"2025-11-07T03:32:38.793249Z","iopub.status.idle":"2025-11-07T03:32:38.803404Z","shell.execute_reply.started":"2025-11-07T03:32:38.793225Z","shell.execute_reply":"2025-11-07T03:32:38.802601Z"},"papermill":{"duration":0.029915,"end_time":"2025-10-23T00:23:19.552693","exception":false,"start_time":"2025-10-23T00:23:19.522778","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"prediction\n0    641\n1    359\nName: count, dtype: int64"},"metadata":{}}],"execution_count":27}]}